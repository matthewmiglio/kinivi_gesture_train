{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cef4743",
   "metadata": {},
   "source": [
    "## Create Keypoint Dataset - Instructions\n",
    "1. Prepare a video of the hand gesture\n",
    "    -Ideally, the video contains only your target gesture. Cut the video to exclude moments before or after your gesture\n",
    "2. Use the final cell to specify your video path. \n",
    "3. Adjust the gesture ids values to match your target dataset\n",
    "4. Running the log_keypoints_from_video_or_folder() script will use mediapipe hand tracking to infer keypoint positions, and add that classificaiton to keypoint.csv\n",
    "5. Use train-keypoint-classifier\\isolated_data\\assembler.ipynb and train-keypoint-classifier\\isolated_data\\analysis.ipynb to combine gesture csvs into a single training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d3a71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# third-party imports\n",
    "import csv\n",
    "import copy\n",
    "import itertools\n",
    "import os\n",
    "from collections import deque\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba50448",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the runtime models\n",
    "from runtime_models.keypoint_classifier.keypoint_classifier import KeyPointClassifier\n",
    "from runtime_models.point_history_classifier.point_history_classifier import PointHistoryClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042c6a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tells mediapipe to use static image mode, or video stream mode (for temportal tracking of keypoints)\n",
    "USE_STATIC_IMAGE_MODE = False \n",
    "MIN_DETECTION_CONFIDENCE = 0.6 \n",
    "MIN_TRACKING_CONFIDENCE = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fc257f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper methods\n",
    "def read_keypoint_classifier_labels():\n",
    "    with open(\n",
    "        \"runtime_models/keypoint_classifier/keypoint_classifier_label.csv\",\n",
    "        encoding=\"utf-8-sig\",\n",
    "    ) as f:\n",
    "        keypoint_classifier_labels = csv.reader(f)\n",
    "        keypoint_classifier_labels = [row[0] for row in keypoint_classifier_labels]\n",
    "        return keypoint_classifier_labels\n",
    "\n",
    "\n",
    "def read_point_history_classifier_labels():\n",
    "    with open(\n",
    "        \"runtime_models/point_history_classifier/point_history_classifier_label.csv\",\n",
    "        encoding=\"utf-8-sig\",\n",
    "    ) as f:\n",
    "        point_history_classifier_labels = csv.reader(f)\n",
    "        point_history_classifier_labels = [\n",
    "            row[0] for row in point_history_classifier_labels\n",
    "        ]\n",
    "        return point_history_classifier_labels\n",
    "\n",
    "\n",
    "def avi_to_images(video_file):\n",
    "    images = []\n",
    "    cap = cv.VideoCapture(video_file)\n",
    "    frame_count = 0\n",
    "    while True:\n",
    "        ret, image = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        images.append(image)\n",
    "\n",
    "        frame_count += 1\n",
    "    cap.release()\n",
    "    return images\n",
    "\n",
    "\n",
    "def calc_landmark_list(image, landmarks):\n",
    "    image_width, image_height = image.shape[1], image.shape[0]\n",
    "\n",
    "    landmark_point = []\n",
    "\n",
    "    for _, landmark in enumerate(landmarks.landmark):\n",
    "        landmark_x = min(int(landmark.x * image_width), image_width - 1)\n",
    "        landmark_y = min(int(landmark.y * image_height), image_height - 1)\n",
    "\n",
    "        landmark_point.append([landmark_x, landmark_y])\n",
    "\n",
    "    return landmark_point\n",
    "\n",
    "\n",
    "def pre_process_landmark(landmark_list):\n",
    "    temp_landmark_list = copy.deepcopy(landmark_list)\n",
    "\n",
    "    base_x, base_y = 0, 0\n",
    "    for index, landmark_point in enumerate(temp_landmark_list):\n",
    "        if index == 0:\n",
    "            base_x, base_y = landmark_point[0], landmark_point[1]\n",
    "\n",
    "        temp_landmark_list[index][0] = temp_landmark_list[index][0] - base_x\n",
    "        temp_landmark_list[index][1] = temp_landmark_list[index][1] - base_y\n",
    "\n",
    "    temp_landmark_list = list(itertools.chain.from_iterable(temp_landmark_list))\n",
    "\n",
    "    max_value = max(list(map(abs, temp_landmark_list)))\n",
    "\n",
    "    def normalize_(n):\n",
    "        return n / max_value\n",
    "\n",
    "    temp_landmark_list = list(map(normalize_, temp_landmark_list))\n",
    "\n",
    "    return temp_landmark_list\n",
    "\n",
    "\n",
    "def pre_process_point_history(image, point_history):\n",
    "    image_width, image_height = image.shape[1], image.shape[0]\n",
    "\n",
    "    temp_point_history = copy.deepcopy(point_history)\n",
    "\n",
    "    base_x, base_y = 0, 0\n",
    "    for index, point in enumerate(temp_point_history):\n",
    "        if index == 0:\n",
    "            base_x, base_y = point[0], point[1]\n",
    "\n",
    "        temp_point_history[index][0] = (\n",
    "            temp_point_history[index][0] - base_x\n",
    "        ) / image_width\n",
    "        temp_point_history[index][1] = (\n",
    "            temp_point_history[index][1] - base_y\n",
    "        ) / image_height\n",
    "\n",
    "    temp_point_history = list(itertools.chain.from_iterable(temp_point_history))\n",
    "\n",
    "    return temp_point_history\n",
    "\n",
    "\n",
    "def count_zeros(list):\n",
    "    zero_count = 0\n",
    "    zero_examples = [0, 0.0, \"0\", \"0.0\", [0.0, 0.0]]\n",
    "    for i in list:\n",
    "        if i in zero_examples:\n",
    "            zero_count += 1\n",
    "    return zero_count\n",
    "\n",
    "\n",
    "def log(\n",
    "    gesture_number,\n",
    "    landmark_list,\n",
    "    print_mode=True,\n",
    "    write_mode=True,\n",
    "):\n",
    "    csv_path = \"keypoint.csv\"\n",
    "    row_content = [gesture_number, *landmark_list]\n",
    "\n",
    "    if print_mode:\n",
    "        print(row_content)\n",
    "\n",
    "    if write_mode:\n",
    "        with open(csv_path, \"a\", newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(row_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64966973",
   "metadata": {},
   "outputs": [],
   "source": [
    "#main logging method\n",
    "def log_keypoints_from_video_or_folder(path, gesture_number=-1):\n",
    "    \"\"\"\n",
    "    logging_mode:\n",
    "        0: No logging\n",
    "        1: Keypoint classifier logging\n",
    "        2: Point history classifier logging\n",
    "    gesture_number:\n",
    "        the number of the gesture that we're adding to the data file\n",
    "    \"\"\"\n",
    "\n",
    "    mp_hands = mp.solutions.hands\n",
    "    hands = mp_hands.Hands(\n",
    "        static_image_mode=USE_STATIC_IMAGE_MODE,\n",
    "        max_num_hands=1,\n",
    "        min_detection_confidence=MIN_DETECTION_CONFIDENCE,\n",
    "        min_tracking_confidence=MIN_TRACKING_CONFIDENCE,\n",
    "    )\n",
    "    keypoint_classifier = KeyPointClassifier()\n",
    "    point_history_classifier = PointHistoryClassifier()\n",
    "    history_length = 16\n",
    "    point_history = deque(maxlen=history_length)\n",
    "    finger_gesture_history = deque(maxlen=history_length)\n",
    "\n",
    "    # load the images depending if folder or video file\n",
    "    if os.path.isdir(path):\n",
    "        print(f\"Processing this path as a folder: {path}\")\n",
    "        image_files = sorted([f for f in os.listdir(path)])\n",
    "        image_paths = [os.path.join(path, image_file) for image_file in image_files]\n",
    "        images = [cv.imread(image_path) for image_path in image_paths]\n",
    "    elif os.path.isfile(path):\n",
    "        print(f\"Processing this path as a video file: {path}\")\n",
    "        images = avi_to_images(path)\n",
    "    else:\n",
    "        print(f\"WARNING! This path is neither a folder nor a file: {path}\")\n",
    "        return\n",
    "\n",
    "    # play video once\n",
    "    for image in images:\n",
    "        image = cv.flip(image, 1)\n",
    "        debug_image = copy.deepcopy(image)\n",
    "\n",
    "        image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "\n",
    "        image.flags.writeable = False\n",
    "        results = hands.process(image)\n",
    "        image.flags.writeable = True\n",
    "\n",
    "        if results.multi_hand_landmarks is not None:\n",
    "            for hand_landmarks, handedness in zip(\n",
    "                results.multi_hand_landmarks, results.multi_handedness\n",
    "            ):\n",
    "                landmark_list = calc_landmark_list(debug_image, hand_landmarks)\n",
    "\n",
    "                pre_processed_landmark_list = pre_process_landmark(landmark_list)\n",
    "                pre_processed_point_history_list = pre_process_point_history(\n",
    "                    debug_image, point_history\n",
    "                )\n",
    "                point_history_csv_path = \"point_history.csv\"\n",
    "\n",
    "                log(\n",
    "                    gesture_number,\n",
    "                    pre_processed_landmark_list,\n",
    "                )\n",
    "\n",
    "                hand_sign_id = keypoint_classifier(pre_processed_landmark_list)\n",
    "\n",
    "                if hand_sign_id == 2:\n",
    "                    point_history.append(landmark_list[8])\n",
    "                else:\n",
    "                    point_history.append([0, 0])\n",
    "\n",
    "                finger_gesture_id = 0\n",
    "                point_history_len = len(pre_processed_point_history_list)\n",
    "                if point_history_len == (history_length * 2):\n",
    "                    finger_gesture_id = point_history_classifier(\n",
    "                        pre_processed_point_history_list\n",
    "                    )\n",
    "\n",
    "                finger_gesture_history.append(finger_gesture_id)\n",
    "        else:\n",
    "            point_history.append([0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f420e94",
   "metadata": {},
   "source": [
    "Here, input the labels for each icon\n",
    "\n",
    "These labels will be used to label these hand-point datums as their respective class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c0edd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_id = 0\n",
    "closed_id = 1\n",
    "cam_point_id = 2\n",
    "vert_point_id = 3\n",
    "selected_id = open_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ba373a",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = r\"C:\\my_files\\data\\matt_gesture\\labeled_videos\\vertical_point_video_2_V123.avi\"\n",
    "log_keypoints_from_video_or_folder(video_path,gesture_number=selected_id)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
